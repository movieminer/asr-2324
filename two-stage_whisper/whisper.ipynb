{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/vol/tensusers3/jdusseljee/hf_home'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class paths:\n",
    "  full_audio_dir = '/vol/tensusers3/jdusseljee/2324-asr/datasets/addresso/train/concatenated'\n",
    "  transcript_dir = '/vol/tensusers3/jdusseljee/2324-asr/datasets/addresso/train/whisper_transcript'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:5\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    }
   ],
   "source": [
    "result = pipe('../datasets/addresso/train/concatenated/S001.wav', generate_kwargs={\"language\": \"english\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/108 [00:00<?, ?it/s]Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "  3%|▎         | 3/108 [00:08<05:22,  3.07s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "  8%|▊         | 9/108 [00:23<04:13,  2.56s/it]--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3216727/2995302658.py\", line 5, in <module>\n",
      "    result = pipe(f'{paths.full_audio_dir}/{file}')\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 285, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"/vol/tensusers3/jdusseljee/2324-asr/.venv/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n",
      "  9%|▉         | 10/108 [00:25<03:55,  2.40s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      " 21%|██▏       | 23/108 [01:00<03:41,  2.61s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      " 51%|█████     | 55/108 [02:31<02:11,  2.49s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      " 54%|█████▎    | 58/108 [02:41<02:39,  3.19s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      " 67%|██████▋   | 72/108 [03:18<01:26,  2.41s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      " 68%|██████▊   | 73/108 [03:21<01:40,  2.86s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      " 69%|██████▉   | 75/108 [03:29<01:54,  3.46s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      " 77%|███████▋  | 83/108 [03:58<01:13,  2.95s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      " 80%|███████▉  | 86/108 [04:06<01:04,  2.95s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      " 82%|████████▏ | 89/108 [04:18<01:06,  3.52s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      " 88%|████████▊ | 95/108 [04:36<00:42,  3.29s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      " 99%|█████████▉| 107/108 [05:09<00:03,  3.11s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "100%|██████████| 108/108 [05:13<00:00,  2.90s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(paths.transcript_dir):\n",
    "  os.makedirs(paths.transcript_dir)\n",
    "\n",
    "for file in tqdm(os.listdir(paths.full_audio_dir)):\n",
    "  result = pipe(f'{paths.full_audio_dir}/{file}')\n",
    "\n",
    "  with open(f\"{paths.transcript_dir}/{file.split('.')[0]}.json\", 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
